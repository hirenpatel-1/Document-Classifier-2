{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for pretty print to all vars\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\" #default=\"last_expr\"\n",
    "\n",
    "#to_display_any_htmls\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StorParam=dict()\n",
    "StorParam['dataset']=dict()\n",
    "StorParam['splits']=dict()\n",
    "StorParam['hparams']=dict()\n",
    "StorParam['metrics']=dict()\n",
    "StorParam['filesMisClf']=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADD', 'BUY', 'EPG']  Total files read: 710\n"
     ]
    }
   ],
   "source": [
    "#Change dir to read dataset files from zip's extract\n",
    "import os\n",
    "os.chdir('C:/Users/mihir.parikh/PyNb/wk_data_op/')\n",
    "files=list()\n",
    "categories=[fl for fl in os.listdir('.') if os.path.isdir(fl)]\n",
    "for folder in categories:\n",
    "        files+=[folder+'/'+p for p in os.listdir(folder)]\n",
    "print(categories,\" Total files read:\",len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 files ignored. New Dataset has 706 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['EPG/45,911.pdf.txt',\n",
       " 'EPG/171006_Protections.pdf.txt',\n",
       " 'EPG/45,910.pdf.txt',\n",
       " 'BUY/reg-level_19CSR-RS30-61-105_The Day Care Provider and Other Day Care Personnel.xml.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess file's text_contents before laoding to pandas dataframe\n",
    "import re\n",
    "dataF={'content':[],'catg':[]}\n",
    "newfiles=[]\n",
    "#adding to dataFrame after preprocessing:\n",
    "#    [remove 1st few not null lines, remove files of less than 10 chars, merge concurrent spaces lines,etc]\n",
    "skip1stLines=2\n",
    "for fi in files:\n",
    "    with open(fi,encoding=\"utf8\") as f:\n",
    "        #print(\"for file\",fi)\n",
    "        PREPROC=f.read()\n",
    "        #make non-space similar to space\n",
    "        PREPROC=PREPROC.replace('Â ',' ') #care...left side wala asli space nai hai\n",
    "        while PREPROC.count('  '):PREPROC=PREPROC.replace('  ',' ')\n",
    "        while PREPROC.count('\\n '):PREPROC=PREPROC.replace('\\n ','\\n')\n",
    "        while PREPROC.count(' \\n'):PREPROC=PREPROC.replace(' \\n','\\n')\n",
    "        while PREPROC.count('\\n\\n'):PREPROC=PREPROC.replace('\\n\\n','\\n')\n",
    "        PREPROC=' '.join(PREPROC.split('\\n')[0 if fi.startswith('Others') else skip1stLines:])\n",
    "        #PREPROC=re.sub(r\"\\d+(\\.\\d*)?\", r\"\",PREPROC) #remove all numbers (digits)\n",
    "        PREPROC=re.sub(r\"[(](\\S*?)[)]\", r\"\",PREPROC) #remove all b/w (brackets)\n",
    "        PREPROC=re.sub(r\"[^a-zA-Z ]\", r\"\",PREPROC) #remove all non alpha (brackets)\n",
    "        while PREPROC.count('  '):PREPROC=PREPROC.replace('  ',' ')\n",
    "        if len(PREPROC)<10:continue\n",
    "        dataF['content'].append(PREPROC)\n",
    "        dataF['catg'].append(fi.split('/')[-2])\n",
    "        newfiles.append(fi)\n",
    "skippedfiles=list(set(files)-set(newfiles))\n",
    "print(len(skippedfiles),\"files ignored. New Dataset has\",len(newfiles),\"files.\")\n",
    "##check skip files of some catg\n",
    "#sorted(list(filter((lambda st:st.startswith('MC')),skippedfiles)))\n",
    "\n",
    "StorParam['dataset']['files']=files\n",
    "StorParam['dataset']['newfiles']=newfiles\n",
    "StorParam['dataset']['preproc_comment']=\"default\"\n",
    "\n",
    "skippedfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPG    291\n",
      "ADD    261\n",
      "BUY    154\n",
      "Name: catg, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catg</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>EPG</td>\n",
       "      <td>Wasbtngton October MEMORANDUM FOR ALL COMPONEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>EPG</td>\n",
       "      <td>UNITED STATES DISTRICT COURT FOR THE DISTRICT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>EPG</td>\n",
       "      <td>UNITED STATES COURT OF APPEALS FOR THE NINTH C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>EPG</td>\n",
       "      <td>UNITED STATES COURT OF APPEALS FOR THE NINTH C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>EPG</td>\n",
       "      <td>United States Court of Appeals For the Seventh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catg</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADD</td>\n",
       "      <td>MIDDLE DISTRICT OF FLORIDA TAMPA DIVISION KENN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADD</td>\n",
       "      <td>UNITED STATES DISTRICT COURT EASTERN DISTRICT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADD</td>\n",
       "      <td>DISTRICT OF MINNESOTA James Chavira Plaintiff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADD</td>\n",
       "      <td>UNITED STATES COURT OF APPEALS TENTH CIRCUIT K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADD</td>\n",
       "      <td>DISTRICT OF KANSAS Equal Employment Opportunit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catg</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>BUY</td>\n",
       "      <td>CFR The average unweighted amount and average...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>BUY</td>\n",
       "      <td>IN THE SUPREME COURT OF PENNSYLVANIA WESTERN D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>BUY</td>\n",
       "      <td>IN THE SUPREME COURT OF PENNSYLVANIA WESTERN D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>BUY</td>\n",
       "      <td>RELATES TO KRS Chapter STATUTORY AUTHORITY KRS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>BUY</td>\n",
       "      <td>January Term FILED June No released at pm RORY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load to pandas dataframe and have a peek on each catg\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(dataF)\n",
    "print(df['catg'].value_counts())\n",
    "for k in df['catg'].value_counts().keys():display(HTML(df[(df['catg']==k)].head(5).to_html()))\n",
    "\n",
    "StorParam['dataset']['catg_val_count']=df['catg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [(353,), (353,)] Test [(211,), (211,)] CV [(142,), (142,)]\n"
     ]
    }
   ],
   "source": [
    "Randoms=(42,420)\n",
    "SplitRatios=(.5,.3,.2) #train test cv\n",
    "#partial import of sklearn. will full load sub-modules as needed.\n",
    "import sklearn\n",
    "#split data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df['content']\n",
    "Y=df['catg']\n",
    "#SplitRatio_ts=0.4\n",
    "SplitRatio_ts=sum(SplitRatios[1:])#1-SplitRatios[0]\n",
    "#SplitRatio_cv=0.5\n",
    "SplitRatio_cv=SplitRatios[-1]/SplitRatio_ts\n",
    "Random_ts,Random_cv=Randoms\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=SplitRatio_ts,random_state=Random_ts)\n",
    "X_test,X_cv,Y_test,Y_cv=sklearn.model_selection.train_test_split(X_test,Y_test,test_size=SplitRatio_cv,random_state=Random_cv)\n",
    "print(\"Train\",[X_train.shape,Y_train.shape],\"Test\",[X_test.shape,Y_test.shape],\"CV\",[X_cv.shape,Y_cv.shape])\n",
    "#now data is ready\n",
    "StorParam['splits']['Ratios']=SplitRatios\n",
    "StorParam['splits']['Randoms']=Randoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method One-shot func: fitPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipelining\n",
    "from sklearn.pipeline import Pipeline\n",
    "def fitPipe(algoname,text_clf_):\n",
    "    global StorParam,X_train,X_test,Y_train,Y_test,X_cv,Y_cv\n",
    "    StorParam['hparams'][algoname]={x:text_clf_.named_steps[x].get_params() for x in text_clf_.named_steps.keys()}\n",
    "    #fit_transforms of data to pipeline\n",
    "    fit_=text_clf_.fit(X_train,Y_train)\n",
    "\n",
    "    #testing model\n",
    "    print(\"--> Accuracy\")\n",
    "    \n",
    "    TRpredicted_ = text_clf_.predict(X_train)\n",
    "    #trscr_=text_clf_.score(X_train,Y_train)\n",
    "    crtab_tr=pd.crosstab(Y_train,TRpredicted_,rownames=[\"True\"],colnames=[\"Train_Predicted\"],margins=True)\n",
    "    trscr_=1-(crtab_tr.All.All-sum([crtab_tr[c][c] for c in text_clf_.classes_]))/crtab_tr.All.All\n",
    "    print(\"TRAIN\",trscr_)\n",
    "\n",
    "    TSpredicted_ = text_clf_.predict(X_test)\n",
    "    #tsscr_=text_clf_.score(X_test,Y_test)\n",
    "    crtab_ts=pd.crosstab(Y_test,TSpredicted_,rownames=[\"True\"],colnames=[\"Test_Predicted\"],margins=True)\n",
    "    tsscr_=1-(crtab_ts.All.All-sum([crtab_ts[c][c] for c in text_clf_.classes_]))/crtab_ts.All.All\n",
    "    print(\"TEST\",tsscr_)\n",
    "    \n",
    "    StorParam['metrics'][algoname]={'train':trscr_,'test':tsscr_,'cv':None}\n",
    "    StorParam['filesMisClf'][algoname]={'train':list(),'test':list(),'cv':list()}\n",
    "\n",
    "    #misClassifications\n",
    "    print(\"--> Train MisClassification\")\n",
    "    misClTr_=list()\n",
    "    #sklearn.metrics.confusion_matrix\n",
    "    display(HTML(crtab_tr.to_html()))\n",
    "    for i in range(len(TRpredicted_)):\n",
    "        if TRpredicted_[i]!=Y_train.iloc[i]:\n",
    "            print(\"X_Train.values[\",i,\"] Predicted\",TRpredicted_[i],\"Actual\",Y_train.iloc[i],\n",
    "                  \"File:[errI=\"+str(X_train.index[i]),newfiles[X_train.index[i]],\"]\")\n",
    "            misClTr_.append(i)\n",
    "            StorParam['filesMisClf'][algoname]['train'].append(newfiles[X_train.index[i]])\n",
    "    print(len(misClTr_),\" misses in TrainData\")\n",
    "\n",
    "    print(\"--> Test MisClassification\")\n",
    "    misClTs_=list()\n",
    "    #sklearn.metrics.confusion_matrix\n",
    "    display(HTML(crtab_ts.to_html()))\n",
    "    for i in range(len(TSpredicted_)):\n",
    "        if TSpredicted_[i]!=Y_test.iloc[i]:\n",
    "            print(\"X_test.values[\",i,\"] Predicted\",TSpredicted_[i],\"Actual\",Y_test.iloc[i],\n",
    "                  \"File:[errI=\"+str(X_test.index[i]),newfiles[X_test.index[i]],\"]\")\n",
    "            misClTs_.append(i)\n",
    "            StorParam['filesMisClf'][algoname]['test'].append(newfiles[X_test.index[i]])\n",
    "\n",
    "    print(len(misClTs_),\" misses in TestData\")\n",
    "    \n",
    "    #push these vars to global namespace\n",
    "    returnD={k+algoname:v for k,v in locals().items() if k.endswith('_')}\n",
    "    for k,v in returnD.items():globals()[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Accuracy\n",
      "TRAIN 1.0\n",
      "TEST 0.8957345971563981\n",
      "--> Train MisClassification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Train_Predicted</th>\n",
       "      <th>ADD</th>\n",
       "      <th>BUY</th>\n",
       "      <th>EPG</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADD</th>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUY</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "      <td>164</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  misses in TrainData\n",
      "--> Test MisClassification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Test_Predicted</th>\n",
       "      <th>ADD</th>\n",
       "      <th>BUY</th>\n",
       "      <th>EPG</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADD</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUY</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPG</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>89</td>\n",
       "      <td>41</td>\n",
       "      <td>81</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.values[ 13 ] Predicted EPG Actual ADD File:[errI=149 ADD/17-120.pdf.txt ]\n",
      "X_test.values[ 31 ] Predicted ADD Actual EPG File:[errI=587 EPG/45752.pdf.txt ]\n",
      "X_test.values[ 37 ] Predicted EPG Actual BUY File:[errI=382 BUY/RocheleauElder.pdf.txt ]\n",
      "X_test.values[ 45 ] Predicted ADD Actual EPG File:[errI=692 EPG/45868.pdf.txt ]\n",
      "X_test.values[ 46 ] Predicted EPG Actual ADD File:[errI=179 ADD/17-150.pdf.txt ]\n",
      "X_test.values[ 58 ] Predicted EPG Actual BUY File:[errI=284 BUY/BuenoLR.pdf.txt ]\n",
      "X_test.values[ 59 ] Predicted EPG Actual ADD File:[errI=133 ADD/17-103.pdf.txt ]\n",
      "X_test.values[ 67 ] Predicted EPG Actual ADD File:[errI=78 ADD/17-043.pdf.txt ]\n",
      "X_test.values[ 95 ] Predicted EPG Actual ADD File:[errI=163 ADD/17-134.pdf.txt ]\n",
      "X_test.values[ 96 ] Predicted EPG Actual BUY File:[errI=338 BUY/MaddenMidland.pdf.txt ]\n",
      "X_test.values[ 101 ] Predicted EPG Actual ADD File:[errI=136 ADD/17-106.pdf.txt ]\n",
      "X_test.values[ 104 ] Predicted ADD Actual EPG File:[errI=674 EPG/45849.pdf.txt ]\n",
      "X_test.values[ 107 ] Predicted EPG Actual ADD File:[errI=73 ADD/17-038.pdf.txt ]\n",
      "X_test.values[ 118 ] Predicted EPG Actual BUY File:[errI=349 BUY/MossFirstPremier.pdf.txt ]\n",
      "X_test.values[ 119 ] Predicted ADD Actual EPG File:[errI=655 EPG/45830.pdf.txt ]\n",
      "X_test.values[ 127 ] Predicted EPG Actual BUY File:[errI=319 BUY/GraySeterus.pdf.txt ]\n",
      "X_test.values[ 128 ] Predicted EPG Actual BUY File:[errI=333 BUY/JohnsonPushpin.pdf.txt ]\n",
      "X_test.values[ 134 ] Predicted EPG Actual BUY File:[errI=344 BUY/McMahonLVNV.pdf.txt ]\n",
      "X_test.values[ 143 ] Predicted EPG Actual BUY File:[errI=367 BUY/PedigoRobertson.pdf.txt ]\n",
      "X_test.values[ 161 ] Predicted ADD Actual EPG File:[errI=481 EPG/45,937.pdf.txt ]\n",
      "X_test.values[ 174 ] Predicted ADD Actual EPG File:[errI=428 EPG/45,882.pdf.txt ]\n",
      "X_test.values[ 204 ] Predicted ADD Actual EPG File:[errI=567 EPG/45732.pdf.txt ]\n",
      "22  misses in TestData\n"
     ]
    }
   ],
   "source": [
    "algoname='nb'\n",
    "#naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_nb = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.01))\n",
    "                    ])\n",
    "fitPipe('nb',text_clf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM SVC [C-Support Vector Classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Accuracy\n",
      "TRAIN 1.0\n",
      "TEST 0.933649289099526\n",
      "--> Train MisClassification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Train_Predicted</th>\n",
       "      <th>ADD</th>\n",
       "      <th>BUY</th>\n",
       "      <th>EPG</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADD</th>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUY</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "      <td>164</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  misses in TrainData\n",
      "--> Test MisClassification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Test_Predicted</th>\n",
       "      <th>ADD</th>\n",
       "      <th>BUY</th>\n",
       "      <th>EPG</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADD</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUY</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPG</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>79</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.values[ 13 ] Predicted EPG Actual ADD File:[errI=149 ADD/17-120.pdf.txt ]\n",
      "X_test.values[ 31 ] Predicted ADD Actual EPG File:[errI=587 EPG/45752.pdf.txt ]\n",
      "X_test.values[ 37 ] Predicted EPG Actual BUY File:[errI=382 BUY/RocheleauElder.pdf.txt ]\n",
      "X_test.values[ 45 ] Predicted ADD Actual EPG File:[errI=692 EPG/45868.pdf.txt ]\n",
      "X_test.values[ 46 ] Predicted EPG Actual ADD File:[errI=179 ADD/17-150.pdf.txt ]\n",
      "X_test.values[ 58 ] Predicted EPG Actual BUY File:[errI=284 BUY/BuenoLR.pdf.txt ]\n",
      "X_test.values[ 95 ] Predicted EPG Actual ADD File:[errI=163 ADD/17-134.pdf.txt ]\n",
      "X_test.values[ 127 ] Predicted EPG Actual BUY File:[errI=319 BUY/GraySeterus.pdf.txt ]\n",
      "X_test.values[ 134 ] Predicted EPG Actual BUY File:[errI=344 BUY/McMahonLVNV.pdf.txt ]\n",
      "X_test.values[ 152 ] Predicted EPG Actual ADD File:[errI=247 ADD/17-218.pdf.txt ]\n",
      "X_test.values[ 174 ] Predicted ADD Actual EPG File:[errI=428 EPG/45,882.pdf.txt ]\n",
      "X_test.values[ 178 ] Predicted EPG Actual ADD File:[errI=108 ADD/17-073.pdf.txt ]\n",
      "X_test.values[ 195 ] Predicted EPG Actual BUY File:[errI=411 BUY/WVCode.mht.txt ]\n",
      "X_test.values[ 204 ] Predicted ADD Actual EPG File:[errI=567 EPG/45732.pdf.txt ]\n",
      "14  misses in TestData\n"
     ]
    }
   ],
   "source": [
    "#pipelining\n",
    "from sklearn.pipeline import Pipeline\n",
    "#svm_svc\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,3), )),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(decision_function_shape=\"ovo\", C = 1000000, kernel='rbf', gamma ='auto'))\n",
    "                    ])\n",
    "fitPipe('svm',text_clf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'splits', 'hparams', 'metrics', 'filesMisClf'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StorParam.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['files', 'newfiles', 'preproc_comment', 'catg_val_count'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StorParam['dataset'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       "   None],\n",
       "  'clf': [SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False)],\n",
       "  'clf__C': [10000, 1000000, 100000, 1, 10, 0.1],\n",
       "  'clf__degree': [5],\n",
       "  'clf__kernel': ['rbf', 'poly', 'sigmoid'],\n",
       "  'vect__ngram_range': [(1, 2), (1, 3), (2, 3)]}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import sklearn.base\n",
    "class Echo(sklearn.base.BaseEstimator,sklearn.base.TransformerMixin):\n",
    "    def transform(self, X, *_):return X\n",
    "    def fit(self, *_):return self\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english',ngram_range=(1,3))),\n",
    "    ('TFidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())                   \n",
    "])\n",
    "clfs=[\n",
    "      { 'clf': [SVC(decision_function_shape=\"ovo\", C = 1000000, kernel='rbf', gamma ='auto')],\n",
    "        'clf__kernel':['rbf','poly','sigmoid'],\n",
    "        'clf__C':[10000,1000000,100000,1,10,0.1],\n",
    "        'clf__degree':[5]\n",
    "      },\n",
    "#      { 'clf': [MultinomialNB(alpha=0.01)],\n",
    "#        'clf__alpha':[0.001,.01,.1,1,10]\n",
    "#      }\n",
    "     ]\n",
    "param_grid = [\n",
    "    {\n",
    "        #'vect__ngram_range': sum([[(i,j) for j in range(i,4+1)] for i in range(1,3+1)],[]),\n",
    "        'vect__ngram_range': [(1,2),(1,3),(2,3)],\n",
    "        'TFidf': [TfidfTransformer()     ,None],#,Echo()],\n",
    "        #'clf': [MultinomialNB(alpha=0.01),SVC(decision_function_shape=\"ovo\", C = 1000000, kernel='rbf', gamma ='auto')],\n",
    "        #'clf__kernel':['rbf','poly'],\n",
    "        #'clf__alpha':np.arange(1.,2.,0.1)\n",
    "        **clf\n",
    "    }\n",
    "for clf in clfs]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid,error_score=0.0,verbose=2)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 324 out of 324 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=0.0,\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 2), (1, 3), (2, 3)], 'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), None], 'clf': [SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=5, gamma='auto', kernel='rbf',\n",
       "  max...l': ['rbf', 'poly', 'sigmoid'], 'clf__C': [10000, 1000000, 100000, 1, 10, 0.1], 'clf__degree': [5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(['abr ']*30,[1,2,3]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=2)]: Done 324 out of 324 | elapsed: 59.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=0.0,\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 2), (1, 3), (2, 3)], 'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), None], 'clf': [SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=5, gamma='auto', kernel='rbf',\n",
       "  m...l': ['rbf', 'poly', 'sigmoid'], 'clf__C': [10000, 1000000, 100000, 1, 10, 0.1], 'clf__degree': [5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed: 81.1min\n",
      "[Parallel(n_jobs=2)]: Done 324 out of 324 | elapsed: 162.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=0.0,\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 2), (1, 3), (2, 3)], 'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), None], 'clf': [SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=5, gamma='auto', kernel='rbf',\n",
       "  m...l': ['rbf', 'poly', 'sigmoid'], 'clf__C': [10000, 1000000, 100000, 1, 10, 0.1], 'clf__degree': [5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362606232294618"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985781990521327"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_cv,Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TFidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovo', degree=5, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'clf__C': 1000000,\n",
       " 'clf__degree': 5,\n",
       " 'clf__kernel': 'rbf',\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_['params'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_TFidf', 'param_clf', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.12135839, 13.513762  , 14.57875641, 13.30735342, 14.10935442,\n",
       "       13.37135347, 13.33268698, 13.35768723, 13.40902114, 13.36135324,\n",
       "       13.5196871 , 13.35501981, 12.28135149, 13.11401947, 12.27368561,\n",
       "       13.10768692, 12.26501942, 12.91701905, 13.29835447, 12.7853535 ,\n",
       "       12.48135217, 13.2270201 , 13.08601936, 12.91111286, 10.84277113,\n",
       "       10.8367637 , 10.90614883, 10.74138546, 11.21613844, 11.01750867,\n",
       "       11.73276567, 11.55197191, 11.449531  , 11.26563096])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_['mean_fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 3, 'error_score': 0.0, 'estimator': Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "         ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]), 'estimator__TFidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'estimator__TFidf__norm': 'l2', 'estimator__TFidf__smooth_idf': True, 'estimator__TFidf__sublinear_tf': False, 'estimator__TFidf__use_idf': True, 'estimator__clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'estimator__clf__alpha': 1.0, 'estimator__clf__class_prior': None, 'estimator__clf__fit_prior': True, 'estimator__memory': None, 'estimator__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('TFidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))], 'estimator__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None), 'estimator__vect__analyzer': 'word', 'estimator__vect__binary': False, 'estimator__vect__decode_error': 'strict', 'estimator__vect__dtype': numpy.int64, 'estimator__vect__encoding': 'utf-8', 'estimator__vect__input': 'content', 'estimator__vect__lowercase': True, 'estimator__vect__max_df': 1.0, 'estimator__vect__max_features': None, 'estimator__vect__min_df': 1, 'estimator__vect__ngram_range': (1,\n",
       "  3), 'estimator__vect__preprocessor': None, 'estimator__vect__stop_words': 'english', 'estimator__vect__strip_accents': None, 'estimator__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'estimator__vect__tokenizer': None, 'estimator__vect__vocabulary': None, 'fit_params': None, 'iid': True, 'n_jobs': 1, 'param_grid': [{'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       "    None],\n",
       "   'clf': [SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)],\n",
       "   'clf__C': [10000, 1000000, 100000, 1, 10, 0.1],\n",
       "   'clf__kernel': ['rbf', 'poly'],\n",
       "   'vect__ngram_range': [(1, 1),\n",
       "    (1, 2),\n",
       "    (1, 3),\n",
       "    (1, 4),\n",
       "    (2, 2),\n",
       "    (2, 3),\n",
       "    (2, 4),\n",
       "    (3, 3),\n",
       "    (3, 4)]},\n",
       "  {'TFidf': [TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       "    None],\n",
       "   'clf': [MultinomialNB(alpha=10, class_prior=None, fit_prior=True)],\n",
       "   'clf__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
       "   'vect__ngram_range': [(1, 1),\n",
       "    (1, 2),\n",
       "    (1, 3),\n",
       "    (1, 4),\n",
       "    (2, 2),\n",
       "    (2, 3),\n",
       "    (2, 4),\n",
       "    (3, 3),\n",
       "    (3,\n",
       "     4)]}], 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': 'warn', 'scoring': None, 'verbose': 0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.get_params('param_grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def approxPIsquared(float_error):\n",
    "\tdenom=1\n",
    "\tmypi=8.0/(denom*denom)\n",
    "\toldpi=0\n",
    "\twhile abs(mypi-oldpi)>=float_error:\n",
    "\t\toldpi=mypi\n",
    "\t\tmypi=mypi+8.0/(denom*denom)\n",
    "\treturn mypi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "approxPIsquared(0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
